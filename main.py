import requests
from bs4 import BeautifulSoup
from collections import Counter
import re
from urllib.parse import urljoin, urlparse

# ä»Šå¤©
#æ˜å¤©
class WebContentAnalyzer:
    """ç¶²é å…§å®¹åˆ†æå™¨"""
    
    def __init__(self, url):
        self.url = url
        self.soup = None
        self.text = ""
        
    def fetch_content(self):
        """æŠ“å–ç¶²é å…§å®¹"""
        try:
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
            response = requests.get(self.url, headers=headers, timeout=10)
            response.raise_for_status()
            response.encoding = response.apparent_encoding
            
            self.soup = BeautifulSoup(response.text, 'html.parser')
            
            # ç§»é™¤ script å’Œ style æ¨™ç±¤
            for script in self.soup(['script', 'style']):
                script.decompose()
            
            self.text = self.soup.get_text()
            return True
        except requests.RequestException as e:
            print(f"âŒ ç„¡æ³•æŠ“å–ç¶²é : {e}")
            return False
    
    def get_title(self):
        """å–å¾—ç¶²é æ¨™é¡Œ"""
        if self.soup:
            title = self.soup.find('title')
            return title.string.strip() if title else "ç„¡æ¨™é¡Œ"
        return None
    
    def count_words(self):
        """çµ±è¨ˆå­—æ•¸"""
        # æ¸…ç†æ–‡æœ¬
        cleaned_text = ' '.join(self.text.split())
        # åˆ†åˆ¥çµ±è¨ˆä¸­æ–‡å­—ç¬¦å’Œè‹±æ–‡å–®è©
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', cleaned_text))
        english_words = len(re.findall(r'\b[a-zA-Z]+\b', cleaned_text))
        
        return {
            'chinese_chars': chinese_chars,
            'english_words': english_words,
            'total_chars': len(cleaned_text)
        }
    
    def extract_keywords(self, top_n=10):
        """æå–é—œéµå­—ï¼ˆè‹±æ–‡ + ä¸­æ–‡ï¼Œç°¡æ˜“è©é »çµ±è¨ˆï¼‰"""
        # æå–è‹±æ–‡å–®è©
        words = re.findall(r'\b[a-zA-Z]{3,}\b', self.text.lower())
        
        # å¸¸è¦‹è‹±æ–‡åœç”¨è©
        en_stop_words = {
            'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all', 
            'can', 'her', 'was', 'one', 'our', 'out', 'this', 'that',
            'with', 'from', 'have', 'has', 'had', 'will', 'more'
        }
        # éæ¿¾è‹±æ–‡åœç”¨è©
        filtered_en_words = [w for w in words if w not in en_stop_words]

        # ä¸­æ–‡æ–·è©ï¼Œä½¿ç”¨ jieba
        try:
            import jieba
            zh_words = list(jieba.cut(self.text))
        except ImportError:
            zh_words = []

        # å¸¸è¦‹ä¸­æ–‡åœç”¨è©ï¼Œå¯æ ¹æ“šéœ€æ±‚æ“´å……
        zh_stop_words = set([
            "çš„", "äº†", "å’Œ", "æ˜¯", "åœ¨", "ä¹Ÿ", "æœ‰", "èˆ‡", "å°‡","åŠ", "æˆ–", "å°±", "éƒ½",
            "è€Œ", "åŠ", "è‘—", "ä»¥", "å°", "ç”±", "åŠå…¶", "ç­‰", "ä¸­", "ä¹‹ä¸€", "ä¸¦"
        ])
        filtered_zh_words = [w for w in zh_words if w.strip() and w not in zh_stop_words and re.match(r'[\u4e00-\u9fff]+', w)]

        # åˆä½µä¸­è‹±æ–‡è©å½™
        all_words = filtered_en_words + filtered_zh_words

        # çµ±è¨ˆè©é »
        word_freq = Counter(all_words)
        return word_freq.most_common(top_n)
    
    def count_links(self):
        """çµ±è¨ˆé€£çµæ•¸é‡"""
        if self.soup:
            all_links = self.soup.find_all('a', href=True)
            external_links = []
            internal_links = []
            
            base_domain = urlparse(self.url).netloc
            
            for link in all_links:
                href = link.get('href')
                full_url = urljoin(self.url, href)
                link_domain = urlparse(full_url).netloc
                
                if link_domain == base_domain:
                    internal_links.append(full_url)
                elif link_domain:
                    external_links.append(full_url)
            
            return {
                'total': len(all_links),
                'internal': len(internal_links),
                'external': len(external_links)
            }
        return None
    
    def count_images(self):
        """çµ±è¨ˆåœ–ç‰‡æ•¸é‡"""
        if self.soup:
            images = self.soup.find_all('img')
            return len(images)
        return 0
    
    def analyze(self):
        """åŸ·è¡Œå®Œæ•´åˆ†æ"""
        print(f"\nğŸ” æ­£åœ¨åˆ†æ: {self.url}\n")
        
        if not self.fetch_content():
            return
        
        # æ¨™é¡Œ
        title = self.get_title()
        print(f"ğŸ“„ æ¨™é¡Œ: {title}")
        print("=" * 60)
        
        # å­—æ•¸çµ±è¨ˆ
        word_count = self.count_words()
        print(f"\nğŸ“Š å­—æ•¸çµ±è¨ˆ:")
        print(f"  â€¢ ä¸­æ–‡å­—ç¬¦: {word_count['chinese_chars']:,}")
        print(f"  â€¢ è‹±æ–‡å–®è©: {word_count['english_words']:,}")
        print(f"  â€¢ ç¸½å­—ç¬¦æ•¸: {word_count['total_chars']:,}")
        
        # é—œéµå­—
        keywords = self.extract_keywords()
        print(f"\nğŸ”‘ Top 10 é—œéµå­—:")
        for i, (word, count) in enumerate(keywords, 1):
            print(f"  {i:2d}. {word:15s} ({count} æ¬¡)")
        
        # é€£çµçµ±è¨ˆ
        links = self.count_links()
        if links:
            print(f"\nğŸ”— é€£çµçµ±è¨ˆ:")
            print(f"  â€¢ ç¸½é€£çµæ•¸: {links['total']}")
            print(f"  â€¢ å…§éƒ¨é€£çµ: {links['internal']}")
            print(f"  â€¢ å¤–éƒ¨é€£çµ: {links['external']}")
        
        # åœ–ç‰‡çµ±è¨ˆ
        image_count = self.count_images()
        print(f"\nğŸ–¼ï¸  åœ–ç‰‡æ•¸é‡: {image_count}")
        
        print("\n" + "=" * 60)
        print("âœ… åˆ†æå®Œæˆï¼\n")


def main():
    """ä¸»ç¨‹å¼"""
    import sys
    import os
    
    print("=" * 60)
    print("  ç¶²é å…§å®¹åˆ†æå™¨ - Web Content Analyzer")
    print("=" * 60)
    
    # æ¸¬è©¦ç”¨ç¶²å€
    default_url = "https://www.python.org"
    
    # å„ªå…ˆé †åºï¼šå‘½ä»¤è¡Œåƒæ•¸ > ç’°å¢ƒè®Šæ•¸ > äº’å‹•è¼¸å…¥ > é è¨­å€¼
    url = None
    
    # 1. æª¢æŸ¥å‘½ä»¤è¡Œåƒæ•¸
    if len(sys.argv) > 1:
        url = sys.argv[1]
    # 2. æª¢æŸ¥ç’°å¢ƒè®Šæ•¸
    elif os.getenv('TARGET_URL'):
        url = os.getenv('TARGET_URL')
    # 3. äº’å‹•è¼¸å…¥
    else:
        try:
            url = input(f"\nè¼¸å…¥è¦åˆ†æçš„ç¶²å€ (ç›´æ¥æŒ‰ Enter ä½¿ç”¨é è¨­: {default_url}): ").strip()
        except (KeyboardInterrupt, EOFError):
            print(f"\nä½¿ç”¨é è¨­ç¶²å€: {default_url}")
            url = ""
    
    if not url:
        url = default_url
    
    # ç¢ºä¿ç¶²å€åŒ…å«å”è­°
    if not url.startswith(('http://', 'https://')):
        url = 'https://' + url
    
    # é–‹å§‹åˆ†æ
    analyzer = WebContentAnalyzer(url)
    analyzer.analyze()


if __name__ == "__main__":
    main()
